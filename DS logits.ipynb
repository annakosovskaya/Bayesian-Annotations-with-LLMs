{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "from pathlib import Path\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS, Predictive\n",
    "from numpyro.ops.indexing import Vindex\n",
    "from tqdm import tqdm\n",
    "from numpyro.handlers import mask\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: P(0) = 0.998, P(1) = 0.002\n",
      "Example 2: P(0) = 0.998, P(1) = 0.002\n",
      "Example 3: P(0) = 0.980, P(1) = 0.020\n",
      "Example 4: P(0) = 0.005, P(1) = 0.995\n",
      "Example 5: P(0) = 0.626, P(1) = 0.374\n",
      "[[0.99847525 0.00152479]\n",
      " [0.9979493  0.00205074]\n",
      " [0.97997653 0.02002344]\n",
      " [0.00530189 0.99469805]\n",
      " [0.6261242  0.37387583]]\n",
      "[[27.625    21.140625]\n",
      " [26.46875  20.28125 ]\n",
      " [26.703125 22.8125  ]\n",
      " [22.90625  28.140625]\n",
      " [24.015625 23.5     ]]\n"
     ]
    }
   ],
   "source": [
    "def get_class_probabilities(logits_path, tokens_json_path, label_tokens=[\"0\", \"1\"]):\n",
    "    \"\"\"\n",
    "    Computes softmax probabilities for class tokens from LLM logits.\n",
    "\n",
    "    Args:\n",
    "        logits_path (str): Path to .npy file containing logits (N x top_k).\n",
    "        tokens_json_path (str): Path to JSON file mapping logits columns to tokens.\n",
    "        label_tokens (list): List of class token strings, e.g. [\"0\", \"1\"].\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (N, 2) with probabilities for class \"0\" and \"1\".\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    logits = np.load(logits_path)\n",
    "    with open(tokens_json_path, \"r\") as f:\n",
    "        token_map = json.load(f)\n",
    "\n",
    "    # Find which columns in logits correspond to the class tokens\n",
    "    label_columns = []\n",
    "    for pos_str, (token, _) in token_map.items():\n",
    "        if token.strip() in label_tokens:\n",
    "            label_columns.append(int(pos_str))\n",
    "\n",
    "    if len(label_columns) != 2:\n",
    "        raise ValueError(f\"Expected 2 class tokens, found {len(label_columns)}: {label_columns}\")\n",
    "\n",
    "    # Extract relevant logits\n",
    "    selected_logits = logits[:, label_columns]\n",
    "\n",
    "    # Softmax function\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # Calculate probabilities\n",
    "    probs = softmax(selected_logits)\n",
    "\n",
    "    return selected_logits, probs\n",
    "\n",
    "logits_train, probs_train = get_class_probabilities(\"outputs/train/logits.npy\", \"outputs/train/top_tokens.json\")\n",
    "logits_val, probs_val = get_class_probabilities(\"outputs/val/logits.npy\", \"outputs/val/top_tokens.json\")\n",
    "\n",
    "# Show first 5 examples\n",
    "for i in range(5):\n",
    "    print(f\"Example {i+1}: P(0) = {probs_train[i, 0]:.3f}, P(1) = {probs_train[i, 1]:.3f}\")\n",
    "\n",
    "print(probs_train[:5])\n",
    "print(logits_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DATA LOADING HELPERS --- #\n",
    "\n",
    "def load_jsonl(file_path, max_items=None):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_items is not None and i >= max_items:\n",
    "                break\n",
    "            data.append(eval(line))\n",
    "    return data\n",
    "\n",
    "def create_annotator_mapping(data):\n",
    "    from collections import defaultdict\n",
    "    annotator_positions = defaultdict(set)\n",
    "    for item in data:\n",
    "        for pos, ann in enumerate(item['annotators']):\n",
    "            annotator_positions[ann].add(pos)\n",
    "    annotator_to_positions = {}\n",
    "    current_position = 0\n",
    "    for annotator in sorted(annotator_positions.keys()):\n",
    "        positions = sorted(annotator_positions[annotator])\n",
    "        for pos in positions:\n",
    "            annotator_to_positions[(annotator, pos)] = current_position\n",
    "            current_position += 1\n",
    "    return annotator_to_positions\n",
    "\n",
    "def process_annotations(data, annotator_mapping=None):\n",
    "    if annotator_mapping is None:\n",
    "        annotator_mapping = create_annotator_mapping(data)\n",
    "\n",
    "    total_positions = max(annotator_mapping.values()) + 1\n",
    "    positions = np.zeros(total_positions, dtype=int)\n",
    "    annotations = np.zeros((len(data), total_positions), dtype=int)\n",
    "    masks = np.zeros((len(data), total_positions), dtype=bool)\n",
    "\n",
    "    for item_idx, item in enumerate(data):\n",
    "        for pos, (annotator, label) in enumerate(zip(item['annotators'], item['labels'])):\n",
    "            if (annotator, pos) in annotator_mapping:\n",
    "                matrix_pos = annotator_mapping[(annotator, pos)]\n",
    "                annotations[item_idx, matrix_pos] = label\n",
    "                masks[item_idx, matrix_pos] = True\n",
    "                positions[matrix_pos] = annotator\n",
    "    return positions, annotations, masks\n",
    "\n",
    "# as result, matrix annotations with columns meaning (annotator, position) and columns meaning labels for each item on (annotator, position)\n",
    "# positions[i] - who annotated the column i in annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DAWID-SKENE MODEL --- #\n",
    "\n",
    "def dawid_skene(positions, annotations, masks, use_llm_prior=False, llm_probs=None):\n",
    "    num_annotators = int(np.max(positions)) + 1\n",
    "    num_classes = int(np.max(annotations)) + 1\n",
    "    num_items, num_positions = annotations.shape\n",
    "\n",
    "    with numpyro.plate(\"annotator\", num_annotators, dim=-2):\n",
    "        with numpyro.plate(\"class\", num_classes):\n",
    "            beta = numpyro.sample(\"beta\", dist.Dirichlet(jnp.ones(num_classes)))\n",
    "    \n",
    "    if use_llm_prior:\n",
    "        assert llm_probs is not None, \"LLM probabilities must be provided if use_llm_prior is True\"\n",
    "        # pi = numpyro.sample(\"pi\", dist.Dirichlet(llm_probs))\n",
    "        #pi = jnp.asarray(llm_probs)\n",
    "        pi = jnp.array(llm_probs[:,np.newaxis,:])  # shape: (num_items, num_classes)\n",
    "\n",
    "    else:\n",
    "        pi = numpyro.sample(\"pi\", dist.Dirichlet(jnp.ones(num_classes)))\n",
    "\n",
    "    with numpyro.plate(\"item\", num_items, dim=-2):\n",
    "        c = numpyro.sample(\"c\", dist.Categorical(probs=pi), infer={\"enumerate\": \"parallel\"})\n",
    "\n",
    "        with numpyro.plate(\"position\", num_positions):\n",
    "            with mask(mask=masks):\n",
    "                numpyro.sample(\n",
    "                    \"y\",\n",
    "                    dist.Categorical(Vindex(beta)[positions, c, :]),\n",
    "                    obs=annotations,\n",
    "                )\n",
    "\n",
    "\n",
    "# --- MAIN EXECUTION --- #\n",
    "\n",
    "def run_ds_on_subset(json_path, use_llm_prior=False, llm_probs=None, max_items=None):\n",
    "    data = load_jsonl(json_path, max_items=max_items)\n",
    "    positions, annotations, masks = process_annotations(data)\n",
    "\n",
    "    if use_llm_prior:\n",
    "        if llm_probs is None:\n",
    "            raise ValueError(\"llm_probs must be provided when use_llm_prior=True\")\n",
    "        if len(llm_probs.shape) != 2:\n",
    "            raise ValueError(f\"llm_probs should be 2D array, got shape {llm_probs.shape}\")\n",
    "        if llm_probs.shape[0] < annotations.shape[0]:\n",
    "            raise ValueError(f\"Not enough LLM probabilities for all items: {llm_probs.shape[0]} < {annotations.shape[0]}\")\n",
    "        llm_probs = llm_probs[:annotations.shape[0]]\n",
    "    \n",
    "    kernel = NUTS(dawid_skene)\n",
    "    mcmc = MCMC(kernel, num_warmup=500, num_samples=1000)\n",
    "    mcmc.run(\n",
    "        random.PRNGKey(0),\n",
    "        positions,\n",
    "        annotations,\n",
    "        masks,\n",
    "        use_llm_prior=use_llm_prior,\n",
    "        llm_probs=llm_probs,\n",
    "    )\n",
    "    mcmc.print_summary()\n",
    "\n",
    "    samples = mcmc.get_samples()\n",
    "    beta_mean = jnp.mean(samples['beta'], axis=0)\n",
    "\n",
    "    print(\"\\nInferred confusion matrices (beta) for annotators:\")\n",
    "    for i, matrix in enumerate(beta_mean):\n",
    "        print(f\"Annotator {i}:\\n{np.round(matrix, 2)}\\n\")\n",
    "\n",
    "    predictive = Predictive(dawid_skene, samples, infer_discrete=True)\n",
    "    discrete_samples = predictive(\n",
    "        random.PRNGKey(1),\n",
    "        positions,\n",
    "        annotations,\n",
    "        masks,\n",
    "        use_llm_prior=use_llm_prior,\n",
    "        llm_probs=llm_probs,\n",
    "    )\n",
    "    predicted_labels = discrete_samples[\"c\"]\n",
    "    return samples, predicted_labels, beta_mean\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#    mcmc_samples, predicted_labels, beta_mean = run_ds_on_subset(\"data/ghc_train.jsonl\", use_llm_prior=True, llm_probs=probs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1500/1500 [00:03<00:00, 469.08it/s, 7 steps of size 3.39e-01. acc. prob=0.92] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      " beta[0,0,0]      0.83      0.12      0.85      0.67      1.00   1557.15      1.00\n",
      " beta[0,0,1]      0.17      0.12      0.15      0.00      0.33   1557.15      1.00\n",
      " beta[0,1,0]      0.30      0.16      0.28      0.02      0.54   1674.09      1.00\n",
      " beta[0,1,1]      0.70      0.16      0.72      0.46      0.98   1674.09      1.00\n",
      " beta[1,0,0]      0.84      0.14      0.89      0.64      1.00   1680.09      1.00\n",
      " beta[1,0,1]      0.16      0.14      0.11      0.00      0.36   1680.09      1.00\n",
      " beta[1,1,0]      0.78      0.18      0.83      0.51      1.00   1747.06      1.00\n",
      " beta[1,1,1]      0.22      0.18      0.17      0.00      0.49   1747.06      1.00\n",
      " beta[2,0,0]      0.83      0.14      0.87      0.64      1.00   1695.99      1.00\n",
      " beta[2,0,1]      0.17      0.14      0.13      0.00      0.36   1695.99      1.00\n",
      " beta[2,1,0]      0.51      0.23      0.52      0.15      0.89   1571.79      1.00\n",
      " beta[2,1,1]      0.49      0.23      0.48      0.11      0.85   1571.79      1.00\n",
      " beta[3,0,0]      0.91      0.08      0.93      0.80      1.00   1619.87      1.00\n",
      " beta[3,0,1]      0.09      0.08      0.07      0.00      0.20   1619.87      1.00\n",
      " beta[3,1,0]      0.43      0.27      0.39      0.02      0.83   1554.25      1.00\n",
      " beta[3,1,1]      0.57      0.27      0.61      0.17      0.98   1554.25      1.00\n",
      " beta[4,0,0]      0.92      0.07      0.94      0.82      1.00   1464.63      1.00\n",
      " beta[4,0,1]      0.08      0.07      0.06      0.00      0.18   1464.63      1.00\n",
      " beta[4,1,0]      0.70      0.24      0.75      0.34      1.00   1197.77      1.00\n",
      " beta[4,1,1]      0.30      0.24      0.25      0.00      0.66   1197.77      1.00\n",
      " beta[5,0,0]      0.74      0.20      0.79      0.43      1.00   1537.35      1.00\n",
      " beta[5,0,1]      0.26      0.20      0.21      0.00      0.57   1537.35      1.00\n",
      " beta[5,1,0]      0.25      0.19      0.20      0.00      0.52   1961.52      1.00\n",
      " beta[5,1,1]      0.75      0.19      0.80      0.48      1.00   1961.52      1.00\n",
      " beta[6,0,0]      0.85      0.09      0.87      0.73      1.00   1588.53      1.00\n",
      " beta[6,0,1]      0.15      0.09      0.13      0.00      0.27   1588.53      1.00\n",
      " beta[6,1,0]      0.41      0.28      0.38      0.00      0.82   1094.96      1.00\n",
      " beta[6,1,1]      0.59      0.28      0.62      0.18      1.00   1094.96      1.00\n",
      " beta[7,0,0]      0.90      0.10      0.92      0.77      1.00   1192.60      1.00\n",
      " beta[7,0,1]      0.10      0.10      0.08      0.00      0.23   1192.60      1.00\n",
      " beta[7,1,0]      0.53      0.16      0.54      0.25      0.79   1615.66      1.00\n",
      " beta[7,1,1]      0.47      0.16      0.46      0.21      0.75   1615.66      1.00\n",
      " beta[8,0,0]      0.93      0.07      0.95      0.83      1.00   2009.38      1.00\n",
      " beta[8,0,1]      0.07      0.07      0.05      0.00      0.17   2009.38      1.00\n",
      " beta[8,1,0]      0.70      0.23      0.75      0.35      1.00   1174.45      1.00\n",
      " beta[8,1,1]      0.30      0.23      0.25      0.00      0.65   1174.45      1.00\n",
      " beta[9,0,0]      0.94      0.04      0.95      0.87      0.99   1534.47      1.00\n",
      " beta[9,0,1]      0.06      0.04      0.05      0.01      0.13   1534.47      1.00\n",
      " beta[9,1,0]      0.63      0.12      0.63      0.44      0.84   1661.83      1.00\n",
      " beta[9,1,1]      0.37      0.12      0.37      0.16      0.56   1661.83      1.00\n",
      "beta[10,0,0]      0.92      0.07      0.94      0.83      1.00   1856.22      1.00\n",
      "beta[10,0,1]      0.08      0.07      0.06      0.00      0.17   1856.22      1.00\n",
      "beta[10,1,0]      0.70      0.24      0.77      0.36      1.00   1225.20      1.00\n",
      "beta[10,1,1]      0.30      0.24      0.23      0.00      0.64   1225.20      1.00\n",
      "beta[11,0,0]      0.95      0.04      0.96      0.89      1.00   1172.88      1.00\n",
      "beta[11,0,1]      0.05      0.04      0.04      0.00      0.11   1172.88      1.00\n",
      "beta[11,1,0]      0.82      0.12      0.85      0.64      0.99   1518.50      1.00\n",
      "beta[11,1,1]      0.18      0.12      0.15      0.01      0.36   1518.50      1.00\n",
      "beta[12,0,0]      0.89      0.10      0.92      0.74      1.00   1366.25      1.00\n",
      "beta[12,0,1]      0.11      0.10      0.08      0.00      0.26   1366.25      1.00\n",
      "beta[12,1,0]      0.86      0.12      0.90      0.68      1.00   1187.29      1.00\n",
      "beta[12,1,1]      0.14      0.12      0.10      0.00      0.32   1187.29      1.00\n",
      "beta[13,0,0]      0.97      0.02      0.98      0.94      1.00   1751.59      1.00\n",
      "beta[13,0,1]      0.03      0.02      0.02      0.00      0.06   1751.59      1.00\n",
      "beta[13,1,0]      0.87      0.07      0.88      0.75      0.97   1968.49      1.00\n",
      "beta[13,1,1]      0.13      0.07      0.12      0.03      0.25   1968.49      1.00\n",
      "beta[14,0,0]      0.66      0.23      0.70      0.31      1.00   1623.54      1.00\n",
      "beta[14,0,1]      0.34      0.23      0.30      0.00      0.69   1623.54      1.00\n",
      "beta[14,1,0]      0.51      0.28      0.52      0.09      0.96   1470.80      1.00\n",
      "beta[14,1,1]      0.49      0.28      0.48      0.04      0.91   1470.80      1.00\n",
      "beta[15,0,0]      0.84      0.15      0.89      0.61      1.00   1833.07      1.00\n",
      "beta[15,0,1]      0.16      0.15      0.11      0.00      0.39   1833.07      1.00\n",
      "beta[15,1,0]      0.62      0.26      0.65      0.21      1.00   1770.30      1.00\n",
      "beta[15,1,1]      0.38      0.26      0.35      0.00      0.79   1770.30      1.00\n",
      "beta[16,0,0]      0.79      0.16      0.83      0.55      1.00   1734.66      1.00\n",
      "beta[16,0,1]      0.21      0.16      0.17      0.00      0.45   1734.66      1.00\n",
      "beta[16,1,0]      0.75      0.20      0.80      0.45      1.00   1811.91      1.00\n",
      "beta[16,1,1]      0.25      0.20      0.20      0.00      0.55   1811.91      1.00\n",
      "beta[17,0,0]      0.92      0.05      0.93      0.85      0.99   2234.82      1.00\n",
      "beta[17,0,1]      0.08      0.05      0.07      0.01      0.15   2234.82      1.00\n",
      "beta[17,1,0]      0.52      0.13      0.52      0.31      0.74   2145.16      1.00\n",
      "beta[17,1,1]      0.48      0.13      0.48      0.26      0.69   2145.16      1.00\n",
      "\n",
      "Number of divergences: 0\n",
      "\n",
      "Inferred confusion matrices (beta) for annotators:\n",
      "Annotator 0:\n",
      "[[0.83       0.17      ]\n",
      " [0.29999998 0.7       ]]\n",
      "\n",
      "Annotator 1:\n",
      "[[0.84 0.16]\n",
      " [0.78 0.22]]\n",
      "\n",
      "Annotator 2:\n",
      "[[0.83       0.17      ]\n",
      " [0.51       0.48999998]]\n",
      "\n",
      "Annotator 3:\n",
      "[[0.90999997 0.09      ]\n",
      " [0.42999998 0.57      ]]\n",
      "\n",
      "Annotator 4:\n",
      "[[0.91999996 0.08      ]\n",
      " [0.7        0.29999998]]\n",
      "\n",
      "Annotator 5:\n",
      "[[0.74 0.26]\n",
      " [0.25 0.75]]\n",
      "\n",
      "Annotator 6:\n",
      "[[0.84999996 0.14999999]\n",
      " [0.41       0.59      ]]\n",
      "\n",
      "Annotator 7:\n",
      "[[0.9        0.09999999]\n",
      " [0.53       0.47      ]]\n",
      "\n",
      "Annotator 8:\n",
      "[[0.93       0.07      ]\n",
      " [0.7        0.29999998]]\n",
      "\n",
      "Annotator 9:\n",
      "[[0.94 0.06]\n",
      " [0.63 0.37]]\n",
      "\n",
      "Annotator 10:\n",
      "[[0.91999996 0.08      ]\n",
      " [0.7        0.29999998]]\n",
      "\n",
      "Annotator 11:\n",
      "[[0.95       0.05      ]\n",
      " [0.82       0.17999999]]\n",
      "\n",
      "Annotator 12:\n",
      "[[0.89       0.11      ]\n",
      " [0.85999995 0.14      ]]\n",
      "\n",
      "Annotator 13:\n",
      "[[0.96999997 0.03      ]\n",
      " [0.87       0.13      ]]\n",
      "\n",
      "Annotator 14:\n",
      "[[0.65999997 0.34      ]\n",
      " [0.51       0.48999998]]\n",
      "\n",
      "Annotator 15:\n",
      "[[0.84 0.16]\n",
      " [0.62 0.38]]\n",
      "\n",
      "Annotator 16:\n",
      "[[0.78999996 0.21      ]\n",
      " [0.75       0.25      ]]\n",
      "\n",
      "Annotator 17:\n",
      "[[0.91999996 0.08      ]\n",
      " [0.52       0.48      ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from numpyro.infer import Predictive\n",
    "\n",
    "# train_data = load_jsonl(\"data/ghc_train.jsonl\", max_items=100)\n",
    "# train_positions, train_annotations, train_masks = process_annotations(train_data)\n",
    "\n",
    "val_data = load_jsonl(\"data/ghc_val.jsonl\", max_items=100)\n",
    "val_positions, val_annotations, val_masks = process_annotations(val_data)\n",
    "\n",
    "train_mcmc_samples, predicted_labels_train, beta_mean_train = run_ds_on_subset(\n",
    "    json_path=\"data/ghc_train.jsonl\",\n",
    "    use_llm_prior=True,\n",
    "    llm_probs=probs_train,\n",
    "    max_items=100\n",
    ")\n",
    "\n",
    "predictive_val = Predictive(\n",
    "    dawid_skene,\n",
    "    posterior_samples={\"beta\": beta_mean_train[None, ...]},  # making shape (1, annotators, C, C)\n",
    "    infer_discrete=True\n",
    ")\n",
    "\n",
    "probs_val = probs_val[:val_annotations.shape[0]]\n",
    "\n",
    "val_pred = predictive_val(\n",
    "    random.PRNGKey(1),\n",
    "    val_positions,\n",
    "    val_annotations,\n",
    "    val_masks,\n",
    "    use_llm_prior=True,\n",
    "    llm_probs=probs_val[:val_annotations.shape[0]]\n",
    ")\n",
    "\n",
    "val_predicted_labels = np.array(val_pred[\"c\"]).squeeze()  # shape (num_val_items,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1500/1500 [00:03<00:00, 472.94it/s, 7 steps of size 3.39e-01. acc. prob=0.92] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      " beta[0,0,0]      0.83      0.12      0.85      0.67      1.00   1557.15      1.00\n",
      " beta[0,0,1]      0.17      0.12      0.15      0.00      0.33   1557.15      1.00\n",
      " beta[0,1,0]      0.30      0.16      0.28      0.02      0.54   1674.09      1.00\n",
      " beta[0,1,1]      0.70      0.16      0.72      0.46      0.98   1674.09      1.00\n",
      " beta[1,0,0]      0.84      0.14      0.89      0.64      1.00   1680.09      1.00\n",
      " beta[1,0,1]      0.16      0.14      0.11      0.00      0.36   1680.09      1.00\n",
      " beta[1,1,0]      0.78      0.18      0.83      0.51      1.00   1747.06      1.00\n",
      " beta[1,1,1]      0.22      0.18      0.17      0.00      0.49   1747.06      1.00\n",
      " beta[2,0,0]      0.83      0.14      0.87      0.64      1.00   1695.99      1.00\n",
      " beta[2,0,1]      0.17      0.14      0.13      0.00      0.36   1695.99      1.00\n",
      " beta[2,1,0]      0.51      0.23      0.52      0.15      0.89   1571.79      1.00\n",
      " beta[2,1,1]      0.49      0.23      0.48      0.11      0.85   1571.79      1.00\n",
      " beta[3,0,0]      0.91      0.08      0.93      0.80      1.00   1619.87      1.00\n",
      " beta[3,0,1]      0.09      0.08      0.07      0.00      0.20   1619.87      1.00\n",
      " beta[3,1,0]      0.43      0.27      0.39      0.02      0.83   1554.25      1.00\n",
      " beta[3,1,1]      0.57      0.27      0.61      0.17      0.98   1554.25      1.00\n",
      " beta[4,0,0]      0.92      0.07      0.94      0.82      1.00   1464.63      1.00\n",
      " beta[4,0,1]      0.08      0.07      0.06      0.00      0.18   1464.63      1.00\n",
      " beta[4,1,0]      0.70      0.24      0.75      0.34      1.00   1197.77      1.00\n",
      " beta[4,1,1]      0.30      0.24      0.25      0.00      0.66   1197.77      1.00\n",
      " beta[5,0,0]      0.74      0.20      0.79      0.43      1.00   1537.35      1.00\n",
      " beta[5,0,1]      0.26      0.20      0.21      0.00      0.57   1537.35      1.00\n",
      " beta[5,1,0]      0.25      0.19      0.20      0.00      0.52   1961.52      1.00\n",
      " beta[5,1,1]      0.75      0.19      0.80      0.48      1.00   1961.52      1.00\n",
      " beta[6,0,0]      0.85      0.09      0.87      0.73      1.00   1588.53      1.00\n",
      " beta[6,0,1]      0.15      0.09      0.13      0.00      0.27   1588.53      1.00\n",
      " beta[6,1,0]      0.41      0.28      0.38      0.00      0.82   1094.96      1.00\n",
      " beta[6,1,1]      0.59      0.28      0.62      0.18      1.00   1094.96      1.00\n",
      " beta[7,0,0]      0.90      0.10      0.92      0.77      1.00   1192.60      1.00\n",
      " beta[7,0,1]      0.10      0.10      0.08      0.00      0.23   1192.60      1.00\n",
      " beta[7,1,0]      0.53      0.16      0.54      0.25      0.79   1615.66      1.00\n",
      " beta[7,1,1]      0.47      0.16      0.46      0.21      0.75   1615.66      1.00\n",
      " beta[8,0,0]      0.93      0.07      0.95      0.83      1.00   2009.38      1.00\n",
      " beta[8,0,1]      0.07      0.07      0.05      0.00      0.17   2009.38      1.00\n",
      " beta[8,1,0]      0.70      0.23      0.75      0.35      1.00   1174.45      1.00\n",
      " beta[8,1,1]      0.30      0.23      0.25      0.00      0.65   1174.45      1.00\n",
      " beta[9,0,0]      0.94      0.04      0.95      0.87      0.99   1534.47      1.00\n",
      " beta[9,0,1]      0.06      0.04      0.05      0.01      0.13   1534.47      1.00\n",
      " beta[9,1,0]      0.63      0.12      0.63      0.44      0.84   1661.83      1.00\n",
      " beta[9,1,1]      0.37      0.12      0.37      0.16      0.56   1661.83      1.00\n",
      "beta[10,0,0]      0.92      0.07      0.94      0.83      1.00   1856.22      1.00\n",
      "beta[10,0,1]      0.08      0.07      0.06      0.00      0.17   1856.22      1.00\n",
      "beta[10,1,0]      0.70      0.24      0.77      0.36      1.00   1225.20      1.00\n",
      "beta[10,1,1]      0.30      0.24      0.23      0.00      0.64   1225.20      1.00\n",
      "beta[11,0,0]      0.95      0.04      0.96      0.89      1.00   1172.88      1.00\n",
      "beta[11,0,1]      0.05      0.04      0.04      0.00      0.11   1172.88      1.00\n",
      "beta[11,1,0]      0.82      0.12      0.85      0.64      0.99   1518.50      1.00\n",
      "beta[11,1,1]      0.18      0.12      0.15      0.01      0.36   1518.50      1.00\n",
      "beta[12,0,0]      0.89      0.10      0.92      0.74      1.00   1366.25      1.00\n",
      "beta[12,0,1]      0.11      0.10      0.08      0.00      0.26   1366.25      1.00\n",
      "beta[12,1,0]      0.86      0.12      0.90      0.68      1.00   1187.29      1.00\n",
      "beta[12,1,1]      0.14      0.12      0.10      0.00      0.32   1187.29      1.00\n",
      "beta[13,0,0]      0.97      0.02      0.98      0.94      1.00   1751.59      1.00\n",
      "beta[13,0,1]      0.03      0.02      0.02      0.00      0.06   1751.59      1.00\n",
      "beta[13,1,0]      0.87      0.07      0.88      0.75      0.97   1968.49      1.00\n",
      "beta[13,1,1]      0.13      0.07      0.12      0.03      0.25   1968.49      1.00\n",
      "beta[14,0,0]      0.66      0.23      0.70      0.31      1.00   1623.54      1.00\n",
      "beta[14,0,1]      0.34      0.23      0.30      0.00      0.69   1623.54      1.00\n",
      "beta[14,1,0]      0.51      0.28      0.52      0.09      0.96   1470.80      1.00\n",
      "beta[14,1,1]      0.49      0.28      0.48      0.04      0.91   1470.80      1.00\n",
      "beta[15,0,0]      0.84      0.15      0.89      0.61      1.00   1833.07      1.00\n",
      "beta[15,0,1]      0.16      0.15      0.11      0.00      0.39   1833.07      1.00\n",
      "beta[15,1,0]      0.62      0.26      0.65      0.21      1.00   1770.30      1.00\n",
      "beta[15,1,1]      0.38      0.26      0.35      0.00      0.79   1770.30      1.00\n",
      "beta[16,0,0]      0.79      0.16      0.83      0.55      1.00   1734.66      1.00\n",
      "beta[16,0,1]      0.21      0.16      0.17      0.00      0.45   1734.66      1.00\n",
      "beta[16,1,0]      0.75      0.20      0.80      0.45      1.00   1811.91      1.00\n",
      "beta[16,1,1]      0.25      0.20      0.20      0.00      0.55   1811.91      1.00\n",
      "beta[17,0,0]      0.92      0.05      0.93      0.85      0.99   2234.82      1.00\n",
      "beta[17,0,1]      0.08      0.05      0.07      0.01      0.15   2234.82      1.00\n",
      "beta[17,1,0]      0.52      0.13      0.52      0.31      0.74   2145.16      1.00\n",
      "beta[17,1,1]      0.48      0.13      0.48      0.26      0.69   2145.16      1.00\n",
      "\n",
      "Number of divergences: 0\n",
      "\n",
      "Inferred confusion matrices (beta) for annotators:\n",
      "Annotator 0:\n",
      "[[0.83       0.17      ]\n",
      " [0.29999998 0.7       ]]\n",
      "\n",
      "Annotator 1:\n",
      "[[0.84 0.16]\n",
      " [0.78 0.22]]\n",
      "\n",
      "Annotator 2:\n",
      "[[0.83       0.17      ]\n",
      " [0.51       0.48999998]]\n",
      "\n",
      "Annotator 3:\n",
      "[[0.90999997 0.09      ]\n",
      " [0.42999998 0.57      ]]\n",
      "\n",
      "Annotator 4:\n",
      "[[0.91999996 0.08      ]\n",
      " [0.7        0.29999998]]\n",
      "\n",
      "Annotator 5:\n",
      "[[0.74 0.26]\n",
      " [0.25 0.75]]\n",
      "\n",
      "Annotator 6:\n",
      "[[0.84999996 0.14999999]\n",
      " [0.41       0.59      ]]\n",
      "\n",
      "Annotator 7:\n",
      "[[0.9        0.09999999]\n",
      " [0.53       0.47      ]]\n",
      "\n",
      "Annotator 8:\n",
      "[[0.93       0.07      ]\n",
      " [0.7        0.29999998]]\n",
      "\n",
      "Annotator 9:\n",
      "[[0.94 0.06]\n",
      " [0.63 0.37]]\n",
      "\n",
      "Annotator 10:\n",
      "[[0.91999996 0.08      ]\n",
      " [0.7        0.29999998]]\n",
      "\n",
      "Annotator 11:\n",
      "[[0.95       0.05      ]\n",
      " [0.82       0.17999999]]\n",
      "\n",
      "Annotator 12:\n",
      "[[0.89       0.11      ]\n",
      " [0.85999995 0.14      ]]\n",
      "\n",
      "Annotator 13:\n",
      "[[0.96999997 0.03      ]\n",
      " [0.87       0.13      ]]\n",
      "\n",
      "Annotator 14:\n",
      "[[0.65999997 0.34      ]\n",
      " [0.51       0.48999998]]\n",
      "\n",
      "Annotator 15:\n",
      "[[0.84 0.16]\n",
      " [0.62 0.38]]\n",
      "\n",
      "Annotator 16:\n",
      "[[0.78999996 0.21      ]\n",
      " [0.75       0.25      ]]\n",
      "\n",
      "Annotator 17:\n",
      "[[0.91999996 0.08      ]\n",
      " [0.52       0.48      ]]\n",
      "\n",
      "Shape of beta samples from training (train_mcmc_samples['beta']): (1000, 18, 2, 2)\n",
      "Shape of c_val_samples (predicted 'c' for validation): (1000, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "from numpyro.infer import Predictive\n",
    "import numpy as np # Ensure numpy is imported if used for post-processing c_val_samples\n",
    "from jax import random # For random.PRNGKey\n",
    "\n",
    "# val_data, val_positions, val_annotations, val_masks are loaded and processed correctly\n",
    "val_data = load_jsonl(\"data/ghc_val.jsonl\", max_items=100)\n",
    "val_positions, val_annotations, val_masks = process_annotations(val_data)\n",
    "\n",
    "# Assume probs_train and probs_val are loaded and processed from previous cells\n",
    "# Ensure probs_train is correctly sliced if its length doesn't match max_items for run_ds_on_subset\n",
    "\n",
    "# Call run_ds_on_subset for training\n",
    "# The 'max_items=100' argument to run_ds_on_subset implies that\n",
    "# your run_ds_on_subset function should be able to pass this to its internal load_jsonl call.\n",
    "# If not, adjust run_ds_on_subset or ensure probs_train aligns with its default loading behavior.\n",
    "train_mcmc_samples, predicted_labels_train_point_estimate, beta_mean_train = run_ds_on_subset(\n",
    "    json_path=\"data/ghc_train.jsonl\",\n",
    "    use_llm_prior=True,\n",
    "    llm_probs=probs_train, # Make sure probs_train corresponds to the items loaded by json_path (e.g., first 100)\n",
    "    max_items=100 # As in your selection, ensure run_ds_on_subset handles this\n",
    ")\n",
    "# predicted_labels_train_point_estimate is based on all samples from this MCMC run itself.\n",
    "\n",
    "# --- Start of changes for validation prediction ---\n",
    "\n",
    "# Create Predictive to get samples of 'c' on the validation set,\n",
    "# using ALL beta samples obtained during training.\n",
    "predictive_for_val_c_samples = Predictive(\n",
    "    dawid_skene,\n",
    "    posterior_samples={'beta': train_mcmc_samples['beta']}, # Use all MCMC beta samples\n",
    "    infer_discrete=True,\n",
    "    return_sites=['c']  # Specify that we want samples for site 'c'\n",
    ")\n",
    "\n",
    "# Prepare probs_val for validation:\n",
    "# Ensure the correct number of items and 2D shape.\n",
    "num_val_items_loaded = val_annotations.shape[0]\n",
    "probs_val_for_pred = probs_val[:num_val_items_loaded]\n",
    "\n",
    "# If probs_val_for_pred happens to be 3D (N, 1, C), squeeze to 2D (N, C)\n",
    "if probs_val_for_pred.ndim == 3 and probs_val_for_pred.shape[1] == 1:\n",
    "    probs_val_for_pred = probs_val_for_pred.squeeze(axis=1)\n",
    "elif probs_val_for_pred.ndim != 2:\n",
    "    # If shape is still not 2D, there's an issue with probs_val preparation\n",
    "    raise ValueError(f\"probs_val_for_pred has an unexpected shape: {probs_val_for_pred.shape}, expected 2D.\")\n",
    "\n",
    "# Get predictive samples for 'c' for the validation set\n",
    "val_c_dist_output = predictive_for_val_c_samples(\n",
    "    random.PRNGKey(1),  # Can use a different key if needed for reproducibility\n",
    "    val_positions,\n",
    "    val_annotations,\n",
    "    val_masks,\n",
    "    use_llm_prior=True,\n",
    "    llm_probs=probs_val_for_pred # Pass the prepared probs_val\n",
    ")\n",
    "\n",
    "# c_val_samples will have shape (num_mcmc_beta_samples, num_validation_items)\n",
    "c_val_samples = val_c_dist_output[\"c\"]\n",
    "\n",
    "# --- End of changes ---\n",
    "\n",
    "# The old code for val_predicted_labels based on beta_mean_train is now replaced by c_val_samples.\n",
    "# If a single array of predicted labels is still needed, it can be derived from c_val_samples, e.g.:\n",
    "# val_predicted_labels_mode = np.array([np.argmax(np.bincount(c_val_samples[:, i].astype(np.int32))) for i in range(c_val_samples.shape[1])])\n",
    "# print(f\"Example val_predicted_labels (mode from c_val_samples): {val_predicted_labels_mode[:10]}\")\n",
    "\n",
    "print(f\"Shape of beta samples from training (train_mcmc_samples['beta']): {train_mcmc_samples['beta'].shape}\")\n",
    "print(f\"Shape of c_val_samples (predicted 'c' for validation): {c_val_samples.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand have the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta_mean_train\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m from the training phase.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m     num_mcmc_draws, num_val_items \u001b[38;5;241m=\u001b[39m c_val_samples\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# num_annotators, num_classes, _ = beta_mean_train.shape # Should be defined from beta_mean_train\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Ensure val_annotations, val_positions, val_masks are also available from previous cells\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_annotations\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_positions\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_masks\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# New cell for evaluating annotator response prediction accuracy\n",
    "import jax.numpy as jnp # Ensure jax.numpy is imported\n",
    "\n",
    "# Check if necessary variables are defined (assuming they are from previous cells)\n",
    "if 'c_val_samples' not in globals() or 'beta_mean_train' not in globals():\n",
    "    print(\"Error: 'c_val_samples' or 'beta_mean_train' not found.\")\n",
    "    print(\"Please ensure you have run the predictions for 'c' on the validation set \")\n",
    "    print(\"and have the 'beta_mean_train' from the training phase.\")\n",
    "else:\n",
    "    num_mcmc_draws, num_val_items = c_val_samples.shape\n",
    "    # num_annotators, num_classes, _ = beta_mean_train.shape # Should be defined from beta_mean_train\n",
    "\n",
    "    # Ensure val_annotations, val_positions, val_masks are also available from previous cells\n",
    "    if 'val_annotations' not in globals() or 'val_positions' not in globals() or 'val_masks' not in globals():\n",
    "        print(\"Error: Validation annotation data (val_annotations, val_positions, val_masks) not found.\")\n",
    "    elif val_annotations.shape[0] != num_val_items:\n",
    "         raise ValueError(f\"Number of items in c_val_samples ({num_val_items}) \"\n",
    "                          f\"does not match val_annotations ({val_annotations.shape[0]})\")\n",
    "    else:\n",
    "        num_val_items_from_annot, num_positions_in_matrix = val_annotations.shape\n",
    "\n",
    "        correct_predictions_count = 0\n",
    "        total_predictions_made = 0\n",
    "\n",
    "        print(f\"Calculating accuracy of predicting annotator responses...\")\n",
    "        print(f\"Using {num_mcmc_draws} MCMC samples for c_val.\")\n",
    "\n",
    "        # Convert to NumPy for the outer loop if jax.jit is not used for the whole block,\n",
    "        # or keep as JAX arrays if the whole computation can be JITted.\n",
    "        # For simplicity here, direct iteration is shown.\n",
    "        \n",
    "        for mcmc_idx in range(num_mcmc_draws):\n",
    "            if (mcmc_idx + 1) % 100 == 0: # Print progress every 100 samples\n",
    "                 print(f\"  Processed MCMC samples: {mcmc_idx + 1}/{num_mcmc_draws}\")\n",
    "\n",
    "            for item_idx in range(num_val_items):\n",
    "                # Get the 'true' label for this item from the current MCMC sample of c\n",
    "                c_true_sample_for_item = c_val_samples[mcmc_idx, item_idx]\n",
    "\n",
    "                for pos_idx in range(num_positions_in_matrix):\n",
    "                    if val_masks[item_idx, pos_idx]: # If there's an actual annotation\n",
    "                        annotator_id = val_positions[pos_idx]\n",
    "                        actual_annotator_label = val_annotations[item_idx, pos_idx]\n",
    "\n",
    "                        # Probability distribution of this annotator's response P(y_ann | c_true, beta_ann)\n",
    "                        # This is beta_mean_train[annotator_id, c_true_sample_for_item, :]\n",
    "                        prob_dist_annotator_response = beta_mean_train[annotator_id, c_true_sample_for_item, :]\n",
    "\n",
    "                        # Make a hard prediction for this annotator's response\n",
    "                        predicted_annotator_label = jnp.argmax(prob_dist_annotator_response)\n",
    "\n",
    "                        if predicted_annotator_label == actual_annotator_label:\n",
    "                            correct_predictions_count += 1\n",
    "                        total_predictions_made += 1\n",
    "        \n",
    "        print(\"Calculation finished.\")\n",
    "\n",
    "        if total_predictions_made > 0:\n",
    "            accuracy_of_predicting_annotator_responses = correct_predictions_count / total_predictions_made\n",
    "            print(f\"\\nAccuracy of predicting annotator responses on validation: {accuracy_of_predicting_annotator_responses:.4f}\")\n",
    "            print(f\"(Based on {total_predictions_made} individual annotator response predictions)\")\n",
    "        else:\n",
    "            print(\"\\nNo predictions were made for annotator responses.\")\n",
    "            print(\"Check val_masks (perhaps all False) or the number of MCMC samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
